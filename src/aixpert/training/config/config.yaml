models:
  - id: "google/gemma-2-9b-it"
    short: "gemma2-9b"
  - id: "Qwen/Qwen2.5-14B-Instruct"
    short: "qwen2.5-14b"
  - id: "meta-llama/Llama-3.2-1B-Instruct"
    short: "llama3.2-1b"
  - id: "google/gemma-2-2b-it"
    short: "gemma2-2b"
  - id: "meta-llama/Meta-Llama-3-8B-Instruct"
    short: "llama3-8b"
  - id: "Qwen/Qwen2-7B-Instruct"
    short: "qwen2-7b"
  - id: "Qwen/Qwen3-8B"
    short: "qwen3-8b"

original_dpo:
  paths:
    train: "src/aixpert/training/data/original/train_finallast.jsonl"
    eval:  "src/aixpert/training/data/original/eval_final.jsonl"
    output_root: "src/aixpert/training/data/original/Models"

  hyperparams:
    max_seq_length: 2048
    load_in_4bit: true
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    batch_size: 2
    grad_accumulation: 16
    num_epochs: 3
    learning_rate: 1.8e-6
    warmup_ratio: 0.25
    beta: 0.1
    save_steps: 100
    logging_steps: 20
    seed: 3407

modified_dpo:
  deltas: [0, 2, 4, 6, 8, 10, 20, 30, 50, 100]

  paths:
    train_file: "src/aixpert/training/data/modified/train_final_flipped.jsonl"
    eval_file:  "src/aixpert/training/data/modified/eval_final_flipped.jsonl"
    output_root: "src/aixpert/training/data/modified/Models"

  hyperparams:
    max_seq_length: 2048
    load_in_4bit: true
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 16
    num_train_epochs: 3
    learning_rate: 1.8e-6
    warmup_ratio: 0.25
    beta: 0.1
    save_steps: 100
    logging_steps: 20

  wandb:
    project: "aixpert"
    entity: "vector-institute-aieng"
    run_prefix: "FactualDPO"
