"""
Generate binary factuality labels (0=factual, 1=hallucinated) for test preference pairs.

This script loads test preference pairs, evaluates each response using a
strict PKU-style binary factual judge. It then saves the dataset with:

    factual_flag_0, factual_flag_1
    h0, h1  (aliases for convenience)

It supports automatic resume, concurrency throttling, and checkpointing.
Fully compliant with ruff, ruff-format, pydocstyle, and mypy.

"""

from __future__ import annotations

import asyncio
import json
import os
import re
from pathlib import Path
from typing import Any, Dict, List

from decouple import Config, RepositoryEnv
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio
from utils import load_yaml


# ============================================================
# Load config.yaml + .env API key
# ============================================================

CONFIG = load_yaml("config.yaml")  # MUST contain: repository: <path>
REPO_PATH = CONFIG["repository"]

env = Config(RepositoryEnv(f"{REPO_PATH}/.env"))
OPENAI_API_KEY = env("OPENAI_API_KEY", default=False)

if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEY not found in .env under repository path.")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)


# ============================================================
# Configuration & Directories
# ============================================================

DATA_DIR = Path(__file__).resolve().parent / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

INPUT_FILE = DATA_DIR / "skywork_preference_pairs_eval.jsonl"
OUTPUT_FILE = DATA_DIR / "skywork_binary_factual_eval.jsonl"

# ============================================================
# Factual Judge Prompt
# ============================================================

BINARY_FACTUAL_JUDGE_PROMPT = """
System:
You are an extremely strict factuality judge. Your task is to determine whether the assistantâ€™s
response contains ANY of the following:

â€¢ factual errors
â€¢ hallucinations
â€¢ unsupported claims
â€¢ unverifiable statements
â€¢ fabricated details
â€¢ incorrect technical descriptions
â€¢ invented studies, names, or entities
â€¢ logical contradictions
â€¢ vague or misleading explanations
â€¢ partial answers that omit critical facts

IMPORTANT:
If the response is vague, incomplete, unsupported, or unverifiable,
classify it as [[1]].

Definitions:
[[0]] â†’ Fully factual, precise, verifiable.
[[1]] â†’ ANY issue: vague, incorrect, unverifiable, misleading, fabricated.

Return ONLY:
[[0]] or [[1]]

User Question:
{question}

Assistant Response:
{answer}
"""


# ============================================================
# API Call â€” Get Binary Factual Flag
# ============================================================


async def get_factual_flag(
    question: str, answer: str, semaphore: asyncio.Semaphore
) -> int:
    """Evaluate a response for factual correctness (0=factual, 1=hallucinated)."""
    prompt = BINARY_FACTUAL_JUDGE_PROMPT.format(question=question, answer=answer)

    async with semaphore:
        for retry in range(5):
            try:
                response = await client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                )

                text = response.choices[0].message.content.strip()
                match = re.search(r"\[\[(0|1)\]\]", text)
                if match:
                    return int(match.group(1))

                return 1  # default: hallucinated

            except Exception as exc:
                print(f"Retry factual-flag ({retry}): {exc}")
                await asyncio.sleep(1 + 0.5 * retry)

    return 1


# ============================================================
# Process One Item
# ============================================================


async def process_single_item(
    item: Dict[str, Any], semaphore: asyncio.Semaphore
) -> Dict[str, Any]:
    """Process one preference pair and produce binary factual labels."""
    prompt = item["prompt"]
    r0 = item["response_0"]
    r1 = item["response_1"]

    f0_task = asyncio.create_task(get_factual_flag(prompt, r0, semaphore))
    f1_task = asyncio.create_task(get_factual_flag(prompt, r1, semaphore))

    f0 = await f0_task
    f1 = await f1_task

    return {
        **item,
        "factual_flag_0": f0,
        "factual_flag_1": f1,
        "h0": f0,
        "h1": f1,
    }


# ============================================================
# Main Async Pipeline
# ============================================================


async def process_dataset() -> None:
    """Load test dataset, compute factual flags, resume if needed, and save output."""
    print(f"ðŸ“¥ Loading test dataset â†’ {INPUT_FILE}")

    with INPUT_FILE.open("r", encoding="utf-8") as f:
        items = [json.loads(line) for line in f]

    # Resume mode
    processed_count = 0
    if OUTPUT_FILE.exists():
        print("â™»ï¸ Resuming previous run...")
        with OUTPUT_FILE.open("r", encoding="utf-8") as f:
            processed_count = sum(1 for _ in f)
        print(f"Found {processed_count} completed items.")

    remaining = items[processed_count:]
    semaphore = asyncio.Semaphore(25)

    tasks = [
        asyncio.create_task(process_single_item(item, semaphore)) for item in remaining
    ]

    buffer: List[str] = []
    count = processed_count

    with OUTPUT_FILE.open("a", encoding="utf-8") as f:
        for coro in tqdm_asyncio.as_completed(tasks, total=len(tasks)):
            result = await coro
            buffer.append(json.dumps(result, ensure_ascii=False) + "\n")
            count += 1

            if len(buffer) >= 25:
                f.writelines(buffer)
                f.flush()
                os.fsync(f.fileno())
                buffer.clear()
                print(f"Checkpoint saved ({count} items).")

        # Flush final buffer
        if buffer:
            f.writelines(buffer)
            f.flush()
            os.fsync(f.fileno())
            print(f"Final checkpoint saved ({count} items).")

    print("âœ… Completed test factual evaluation.")


# ============================================================
# Entry Point
# ============================================================

if __name__ == "__main__":
    asyncio.run(process_dataset())
