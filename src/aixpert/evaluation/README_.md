# AIXpert Preference Alignment â€” Evaluation Pipeline
GPT-4o-mini Judge Â· Factuality Scoring Â· Multi-Model Benchmarking

This directory implements the **automated evaluation pipeline** used to benchmark:

- Original-DPO models
- Factual-DPO++ models (across Î” = 0, 2, 4, 6, 8, 10, 20, 30, 50, 100)

Evaluation is performed using **GPT-4o-mini** as an LLM-as-a-judge.

All evaluation configuration is pulled from:

```
src/aixpert/config/config.yaml
```

---

## ğŸ“ Evaluation Directory Structure

```
src/aixpert/evaluation/
â”‚
â”œâ”€â”€ evaluations/
â”‚   â””â”€â”€ run_all_evaluations.py      # Main orchestrator
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ eval_core_utils.py          # Generation + GPT judge scoring
â”‚   â””â”€â”€ eval_template.py            # Factual judge prompt
```

---

# âš™ï¸ Configuration Overview (Evaluation)

The configuration includes:

---

## 1ï¸âƒ£ Evaluation Settings

```yaml
eval:
  data_file: "src/aixpert/data_construction/data/skywork_extracted_test.jsonl"
  batch_size: 16
  max_new_tokens: 2048
  judge_concurrency: 10
```

---

## 2ï¸âƒ£ Model Paths

```yaml
paths:
  original_root: "src/aixpert/training/data/original/Models"
  modified_root: "src/aixpert/training/data/modified/Models"
```

The evaluation script automatically locates checkpoints:

```
<short>_OriginalDPO/
<short>_delta<value>/
```

---

## 3ï¸âƒ£ Model Registry & Î” Values

```yaml
models:
  - short: "gemma2-9b"
  - short: "llama3-8b"
  - short: "qwen3-8b"
```

```yaml
deltas: [0, 2, 4, 6, 8, 10, 20, 30, 50, 100]
```

Total evaluations:

```
7 models Ã— 10 deltas = 70 comparisons
```

---

## ğŸ§  Factual Judge Model

```yaml
model:
  name: "gpt-4o-mini"
  temperature: 0.8
```

---

# ğŸ“Š Evaluation Metrics

For each model pair, the pipeline computes:

| Metric | Meaning |
|--------|---------|
| factuality_A | Mean factual score of Original-DPO model |
| factuality_B | Mean factual score of Î”-model |
| halluc_rate_A | % outputs scoring < 5 |
| halluc_rate_B | % outputs scoring < 5 |
| win_rate | How often Î”-model outperforms baseline |
| count | Total prompts evaluated |

Results saved to:

```
eval_results.json
```

---

# ğŸš€ Running Evaluation

```bash
python -m aixpert.evaluation.evaluations.run_all_evaluations
```

The script:

1. Loads config
2. Loads evaluation prompts
3. Loads Original-DPO and Î”-models
4. Generates responses
5. Sends to GPT-4o-mini asynchronously
6. Computes metrics
7. Saves results

---

# ğŸ§© Core Components

## `eval_core_utils.py`

Includes:

- **batch_generate()** â†’ Deterministic HF inference
- **judge_factual()** â†’ Scores one answer
- **judge_many()** â†’ Async batch scoring
- **evaluate_pair()** â†’ Full evaluation for one (model, Î”)

---

## `eval_template.py`

Provides the factuality judge prompt using:

```
[[score]]
```

format.

---

# âœ… Summary

This evaluation pipeline provides:

- End-to-end factuality benchmarking
- Async OpenAI judge scoring
- Multi-model Ã— multi-delta evaluation
- Config-driven reproducibility
- Clean JSON output for papers and analysis
