# AIXpert-preference-alignment

# ğŸ§  AIXpert Preference Alignment â€“ DPO Training

This repository contains scripts and configuration files for **Direct Preference Optimization (DPO) fine-tuning** using large language models such as **Qwen2-7B-Instruct**.  
It is designed for **Vector Instituteâ€™s AIXpert project**, supporting reproducible training, evaluation, and checkpointing workflows.

---

## âš™ï¸ Environment Setup

This project uses [**uv**](https://github.com/astral-sh/uv) â€” a fast, modern Python package manager.  
You do **not** need a `requirements.txt` or `uv.lock` file; everything is specified in `pyproject.toml`.

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/VectorInstitute/AIXpert-preference-alignment.git
cd AIXpert-preference-alignment

```
### ğŸ› ï¸ 2ï¸âƒ£ Create and Sync the Environment

Run the following command to automatically create a virtual environment and install all dependencies:

```bash
uv sync
source .venv/bin/activate
```
## ğŸ§® Dataset Construction â€“ `Sky()` Function Overview - preprocess_dataset.py

The `Sky()` function prepares a **pairwise preference dataset** used for **Direct Preference Optimization (DPO)** fine-tuning.  
It converts raw conversation data (stored as a Parquet file) into a **structured, model-ready dataset** with clear questionâ€“answer pairs and preference labels.

---
### ğŸ’¡ Purpose

This step produces the **pairwise preference dataset** required for training and evaluating DPO models.  
It enables the model to learn *which answer is better and why*, forming the foundation for **preference alignment** and **critic inference**.

---

## ğŸŒ Translation Pipeline â€“ `translation.py`

The `translation.py` script automates the **English translation of Chinese preference prompts** in the dataset generated by `Sky()`.  
It uses the **OpenAI GPT-4o-mini model** to perform literal translations while maintaining all JSON structure and field meanings intact.

---

### ğŸ” Step-by-Step Overview

1. **ğŸ”‘ API and Configuration Setup**
   - Loads the **OpenAI API key** securely from the `.env` file using `pydantic-settings`.
   - Initializes the `OpenAI` client for communication with GPT-4o-mini.

2. **ğŸ“¦ Dataset Loading**
   - Loads the processed dataset from:
     ```
     /data/pairwise_critic_inference2_get_answer/Sky
     ```
   - Uses only the first 5,000 samples (`subset = dataset.select(range(5000))`) to manage cost and speed.

3. **ğŸ§  Translation Logic**
   - Each Chinese `prompt` (which contains question and answer text) is passed to the GPT-4o-mini model.
   - The model is instructed to:
     - Translate **literally** into English.
     - **Not** interpret, execute, or change the structure of the text.
     - Preserve any embedded JSON structure.

   Example instruction sent to GPT-4o-mini:


## ğŸ§  Inference Generation â€“ Repeated Sampling

The `inference_best_of_n.py` script performs **multi-sample text generation** using a pretrained language model (e.g., `Qwen2-7B-Instruct`).  
It takes an input dataset of prompts and produces **multiple candidate responses per prompt**, storing results and intermediate checkpoints for safe recovery.

This script is typically the **first step after cloning the repo**, used to run controlled inference before fine-tuning or preference evaluation.

---

### â–¶ï¸ To Run

Once your environment is set up:

```bash
uv run inference_best_of_n.py
```
## ğŸ§© Inference â€“ All Template Generation (Hint Sampling)

Once `inference_best_of_n.py` has generated the multi-sample outputs,  
the next step is to **generate evaluation prompts and structured outputs** across different templates â€” specifically `guide` and `guide_reverse`.  

This script automates the process of converting questionâ€“answer pairs into evaluation-style prompts and uses a pretrained LLM (e.g., `Qwen2-7B-Instruct`) to produce reasoned preference justifications.  
It also includes built-in checkpointing and resume capabilities for long runs.

---

### ğŸš€ Command to Run

```bash
uv run inference_all.py
```
## ğŸ§¹ Post-Processing

After generating the structured outputs (`guide.jsonl`, `guide_reverse.jsonl`, and `output.jsonl`) from the inference scripts,  
this script performs **final cleanup and reindexing** to ensure that all generated files are properly aligned, consistent, and ready for downstream evaluation or DPO fine-tuning.

---

### ğŸš€ Command to Run

```bash
uv run post_processing.py
```
## ğŸ§± DPO Dataset Construction

After cleaning and aligning the inference outputs, this script **constructs the final dataset** required for **Direct Preference Optimization (DPO)** fine-tuning.  
It parses the modelâ€™s reasoning outputs, identifies preference-aligned samples, and builds positiveâ€“negative pairs used to train the reward-aligned model.

---

### ğŸš€ Command to Run

```bash
uv run construct_dpo_dataset.py
```
## ğŸ“Š DPO Dataset Inspection 

After constructing the DPO dataset using `construct_dpo_dataset.py`,  
this script provides a **quick inspection and summary** of the saved dataset â€” verifying that the train/validation splits and pair-type distributions are correct.

---

### ğŸš€ Command to Run

```bash
uv run dpo_dataset.py
```
## ğŸ§  Direct Preference Optimization Training 
This script performs **Direct Preference Optimization (DPO)** fine-tuning on the constructed dataset using **Qwen2-7B-Instruct**.  
It aligns the modelâ€™s responses with human-preferred outputs by learning from **(chosen, rejected)** pairs generated earlier.

---

### ğŸš€ Command to Run

```bash
uv run dpo_training.py
```
## ğŸ“Š Model Evaluation Script 

This script evaluates the **Direct Preference Optimization (DPO)** fine-tuned model against the **base model (Qwen2-7B-Instruct)** on the validation split of the dataset.  
It measures how well each model predicts the **preferred (chosen)** answers from the DPO dataset â€” effectively computing **alignment accuracy**.

---

### ğŸš€ Command to Run

```bash
uv run accuracy.py
```